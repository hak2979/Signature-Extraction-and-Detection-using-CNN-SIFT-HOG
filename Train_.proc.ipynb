{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"\"  # Set your main directory here\n",
    "\n",
    "# Parameters\n",
    "img_size = (256, 256)\n",
    "data_train = []\n",
    "labels_train = []\n",
    "data_test = []\n",
    "labels_test = []\n",
    "\n",
    "# Load training data\n",
    "train_dir = os.path.join(main_dir, \"train\")\n",
    "for folder in os.listdir(train_dir):\n",
    "    folder_path = os.path.join(train_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            data_train.append(img)\n",
    "            labels_train.append(folder)\n",
    "\n",
    "# Load test data\n",
    "test_dir = os.path.join(main_dir, \"test\")\n",
    "for folder in os.listdir(test_dir):\n",
    "    folder_path = os.path.join(test_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            data_test.append(img)\n",
    "            labels_test.append(folder)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data_train = np.array(data_train)\n",
    "labels_train = np.array(labels_train)\n",
    "data_test = np.array(data_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# Normalize\n",
    "data_train = data_train.astype('float32') / 255.0\n",
    "data_test = data_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN\n",
    "X_train = data_train.reshape(-1, img_size[0], img_size[1], 1)\n",
    "X_test = data_test.reshape(-1, img_size[0], img_size[1], 1)\n",
    "\n",
    "# Shuffle training data\n",
    "train_indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(train_indices)\n",
    "X_train = X_train[train_indices]\n",
    "labels_train = labels_train[train_indices]\n",
    "\n",
    "# Shuffle test data (optional)\n",
    "test_indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(test_indices)\n",
    "X_test = X_test[test_indices]\n",
    "labels_test = labels_test[test_indices]\n",
    "\n",
    "# Display first five training imag\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(X_train[i].reshape(img_size[0], img_size[1]), cmap='gray')\n",
    "    plt.title(labels_train[i])\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display first five test imag\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(img_size[0], img_size[1]), cmap='gray')\n",
    "    plt.title(labels_test[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "main_dir = \"\"  \n",
    "\n",
    "# Parameters\n",
    "img_size = (256, 256)\n",
    "batch_size = 32\n",
    "\n",
    "# Create ImageDataGenerator instances for training and testing\n",
    "datagen_train = ImageDataGenerator(rescale=1./255, \n",
    "                                    validation_split=0.4)\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Prepare training dataset\n",
    "train_generator = datagen_train.flow_from_directory(\n",
    "    os.path.join(main_dir, \"train\"),\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  \n",
    "    subset='training'  \n",
    ")\n",
    "\n",
    "# Prepare validation dataset\n",
    "validation_generator = datagen_train.flow_from_directory(\n",
    "    os.path.join(main_dir, \"train\"),  \n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', \n",
    "    subset='validation'  \n",
    ")\n",
    "\n",
    "# Prepare test dataset\n",
    "test_generator = datagen_test.flow_from_directory(\n",
    "    os.path.join(main_dir, \"test\"), \n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  \n",
    ")\n",
    "\n",
    "# Get class names\n",
    "classnames = list(train_generator.class_indices.keys())\n",
    "print('Data generators ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(generator, title):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(5):\n",
    "        img, label = next(generator)\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(img[0])  \n",
    "        plt.title(classnames[label.argmax()])  \n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_images(train_generator, \"First 5 Training Images\")\n",
    "\n",
    "display_images(validation_generator, \"First 5 Validation Images\")\n",
    "\n",
    "display_images(test_generator, \"First 5 Test Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from  tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "# Define the regularization factor (this can be tuned)\n",
    "l2_reg = 1e-2  # Example value, you can adjust this\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "input_shape = train_generator.image_shape\n",
    "\n",
    "model.add(Conv2D(64, (10, 10), input_shape=input_shape, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (10, 10), activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (10, 10), activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (10,10), activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_reg)))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with AdamW optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.AdamW(learning_rate=0.00005, weight_decay=1e-5),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_generator.samples // batch_size\n",
    "validation_steps = validation_generator.samples // batch_size\n",
    "\n",
    "# Define the filepath for saving the best model\n",
    "checkpoint_filepath = 'best_sign_model_de.keras'\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,  # Only save the best model\n",
    "    monitor='val_loss',   # Monitor validation loss\n",
    "    mode='min',           # Save the model with the minimum validation loss\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#model=load_model(\"best_sign_model.keras\")\n",
    "# Train the model and save the best model based on validation loss\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=100,\n",
    "                    callbacks=[checkpoint_callback])  # Include the checkpoint callback\n",
    "\n",
    "# Save the entire model after training (optional)\n",
    "model.save(\"sign_det.keras\")  # Saving as a .keras file\n",
    "\n",
    "# Load the best model after training\n",
    "best_model = load_model(\"best_sign_model_de.keras\")\n",
    "\n",
    "# Print the summary of the best model\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(\"best_sign_model.keras\")\n",
    "test_loss, test_accuracy = best_model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_generator):\n",
    "    # Ensure steps cover all samples\n",
    "    steps = int(np.ceil(test_generator.samples / test_generator.batch_size))\n",
    "\n",
    "    # Predict using the test generator\n",
    "    y_pred = model.predict(test_generator, steps=steps)\n",
    "\n",
    "    # Convert predictions to class labels (argmax for multi-class classification)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Get the true class labels from the test generator\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    # Ensure that y_pred_classes and y_true are the same length\n",
    "    if len(y_pred_classes) != len(y_true):\n",
    "        print(f\"Warning: Mismatch in the number of predictions ({len(y_pred_classes)}) and true labels ({len(y_true)}). Truncating predictions.\")\n",
    "        y_pred_classes = y_pred_classes[:len(y_true)]  # Truncate predictions if necessary\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "    # Print the confusion matrix for reference\n",
    "    print(\"Confusion Matrix (Numeric):\")\n",
    "    print(cm)\n",
    "\n",
    "    # Initialize lists to store TP, TN, FP, FN for each class\n",
    "    summary_data = []\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        TP = cm[i, i]\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        FN = cm[i, :].sum() - TP\n",
    "        TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "        # Calculate Precision and Recall\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "        # Append results to the summary data list\n",
    "        summary_data.append({\n",
    "            'Class': i, \n",
    "            'TP': TP, \n",
    "            'TN': TN, \n",
    "            'FP': FP, \n",
    "            'FN': FN,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        })\n",
    "\n",
    "        # Accumulate totals for overall precision and recall\n",
    "        total_tp += TP\n",
    "        total_fp += FP\n",
    "        total_fn += FN\n",
    "\n",
    "    # Convert summary data to DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # Calculate overall Precision and Recall\n",
    "    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "\n",
    "    # Display the summary table\n",
    "    print(\"\\nSummarized Confusion Matrix:\")\n",
    "    print(summary_df)\n",
    "\n",
    "    # Print overall Precision and Recall\n",
    "    print(f\"\\nOverall Precision: {overall_precision:.4f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "\n",
    "# Call the evaluate_model function with the best_model and test_generator\n",
    "evaluate_model(best_model, test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history using the history returned from model.fit()\n",
    "plot_training_history(history)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
